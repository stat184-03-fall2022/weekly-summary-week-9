---
title: "Weekly Summary Week 9"
author: "Jack Benadon"
title-block-banner: true
title-block-style: default
toc: true
format: html
# format: pdf
---

---

## Tuesday, Jan 17

::: {.callout-important}
## TIL

Include a _very brief_ summary of what you learnt in this class here. 

Today, I learnt the following concepts in class:

1. Recap of last class, using the torch package
1. Decision boundary
1. Multinomial logistical regression
:::

Provide more concrete details here. You can also use footenotes[^footnote] if you like

```{R}
library(dplyr)
library(purrr)
library(glmnet)
library(torch)
library(ISLR2)
library(tidyr)
library(readr)
library(caret)
library(mlbench)
library(nnet)
```

### Logistic loss function using the torch library
also known as Binary cross entropy

```{R}
nn_bce_loss()
```


### Some more things about classification:
#### Decision boundary
The decision boundary is a line that 
```{R}
library(class) 

X <- t(replicate(200, runif(2)))
y <- ifelse(apply(X,1,\(x)sum(x^1.5)) + 0.2 *rnorm(200) <= 1, 0, 1) %>% as.factor()
col <- ifelse(y== 0,"blue", "red")
plot(X[,1], X[,2], col=col)
```

```{R}
df <- data.frame(y=y, x1=X[,1], x2=X[,2])
model <- glm(y~., df, family=binomial())
summary(model)
```

```{R}
xnew <- data.frame(
  x1 = rep(seq(0,1,length.out=50),50),
  x2 = rep(seq(0,1,length.out=50), each = 50)
)

prob <- predict(model, xnew, type = 'response')
decision <- ifelse(prob < 0.5, "blue", "red" )

plot(xnew[,1], xnew[,2], col = decision, pch = 22)
points(X[,1], X[,2], col = col, pch = 20)
        
```

#### Confusion matrix

```{R}
idx <- sample(1:nrow(df),50)
train <- df[-idx, ]
test <- df[idx, ]

model <- glm(y~., train, family=binomial())
probs <- predict(model, test, type="response")

predicted <- ifelse(probs < 0.5, 0,1)
expected <- test$y

table(predicted, expected)

```
the output is a 2x2 table of a binary classification problem. The 0 in the rows are the predictions that we're generating. So out of 37 people the model correctly predicted 33 of them. It gave 4 wrong predictions out of the 37. And in class 1 out of the 13 people 10 of them were correctly predicted to be in class 1. and in 3 cases it wrongly predicted class 0 when it should have been class 1.

```{R}
caret::confusionMatrix(data=as.factor(predicted), reference=as.factor(expected))
```

In this case its straight forward because there are just 2 classes. Its deciding between 0 and 1. How would we do if we had 3 different classes?

### Multinomial logistic regression

#### Softmax function
Similar to the sigmoid function in logistc regression, for multinomial logistic regression we use the softmax function.

$$
\text{soft-max}(x_1, x_2, \dots, x_k) =(\frac{e^x_1}{\sum^k e^x)},\frac{e^x_2}{\sum^k e^x_i},\dots ,\frac{e^x_k}{\sum^k e^x_i}) = 1
$$

Soft-max() can be interpreted as the probabilities associated with each of the input $k$ classes. Given covariates x and response y.






## Thursday, Jan 19



::: {.callout-important}
## TIL

Include a _very brief_ summary of what you learnt in this class here. 

Today, I learnt the following concepts in class:

1. Multinomial logistic regression
1. Classification (decision) tree
1. Support Vector Machine
:::

Provide more concrete details here, e.g., 

```{R}
sample(1:3, size = 1000, replace = TRUE, prob = c(0.8, 0.1,0.1))

```

```{R}
b <- c(-10,0,10)
prob_function = \(x) exp(b*x) / sum(exp(b*x))
```

```{R}
x <- rnorm(1000)
y<- c()
for(i in 1:length (x)) {
  y[i] <-sample(0:2, 1 , prob = prob_function(x[i]))
}
cbind(x,y) %>% head 
```



```{R}
df <- data.frame(x=x, y=as.factor(y))
df$y <- relevel(df$y, ref= "1")
df$y
```


```{R}

model <- multinom(y ~ x,df)
summary(model)
```
```{R}
#I couldn't get the != operator to work.
#n <- 250
#x <- t(replicate(n,2*runif(2)-1))
#y <- ifelse(X,1, \(x) sum(sign(x+0.01 * rnorm(2))) != 0,0,1) %>% as.factor()
#
#col <- ifelse(y = 0, "blue", "red")
#plot(X[,1], X[,2], col=col, pch=19)
```


```{R}
df <- data.frame(y=y, x1=X[,1], x2=X[,2])
model <- glm(y~ x1 +x2, df, family=binomial())
f_logistic = \(x) predict(model, data.frame(x1=x[,1], x2[,2]), type="response")
```

```{R}
xnew <- cbind(
  rep(seq(-1.1,1.1, length.out=50), 50),
  rep(seq(-1.1,1.1, length.out=50), 50)
)
```

```{R}
plt <- function(f,x) {
  plot(x[,1], x[,2], col=ifelse(f(x) < 50, "blue", 'red'), pch=22)
  points(df$x1, df$x2, col =ifelse(y == 0 ))
}
```


### Classification (decision) tree
Classification trees are commonly used in machine learning to predict categorical outcomes. They are a hierarchical model that recursively partitions the data by finding splits in the variables based on the most informative features.

They're very good at predicting what your categories will be. In the class examples it made some errors because it predicted blue when it was actually red. But it was only in 7 instances out of 250.

### Support Vector Machine
Were very popular before neural nets. They're useful because they're designed to construct options. They're known as maximum margin non-linear. The name of the package in r is `(e1071)`. Kernel is one of the parameters used with svm's. It has different types that can really change the result. Some of the types are: radial, polynomial, sigmoid.

When compared to the decision trees, svms do smooth curves while the trees dont do that. 

### Neural Network with 1 hidden Layer
They add more linear layers inside.
```{R}
module <- nn_module(
  initialize = function() {
    self$f <- nn_linear(2,20)
    self$f <- nn_linear(2,20) #hidden layer
    self$f <- nn_linear(2,20) #hidden layer
    self$f <- nn_linear(2,20) #hidden layer
    self$g <- nn_linear(2,1)
    self$h <- nn_sigmoid()
  },
  forward = function(x) {
    x %>%
      self$f() %>%
      self$g() %>%
      self$h()
  }
)
```


[^footnote]: You can include some footnotes here